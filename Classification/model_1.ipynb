{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "V100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training and Fine Tuning of ConvNeXtLarge with augmentation layers\n"
   ],
   "metadata": {
    "id": "ToY-4vBFTHpI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base imports\n"
   ],
   "metadata": {
    "id": "E6efF90wTUto"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEYwzWQmqBh8"
   },
   "outputs": [],
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 90\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhoK_GByqP1x"
   },
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.applications import ConvNeXtLarge\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPool2D,  \\\n",
    "    Dropout, Dense, Input, concatenate,      \\\n",
    "    GlobalAveragePooling2D, AveragePooling2D,\\\n",
    "    Flatten\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZP6GvRzKqVv4"
   },
   "outputs": [],
   "source": [
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "id": "eJ8CEVmhLBwW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset with rotate/flip augmentation"
   ],
   "metadata": {
    "id": "5LwZacXITaEV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGJ6ZRzpqdzA"
   },
   "outputs": [],
   "source": [
    "dataset = np.load('public_data/OptimizedDatasets/train_dataset_aug_mix.npz', allow_pickle=True)\n",
    "keys = dataset.keys()\n",
    "\n",
    "# Print the keys to see what is inside the dataset\n",
    "print(\"Keys in the dataset:\", keys)\n",
    "\n",
    "# Access individual arrays/objects using the keys and print their shapes or values if needed\n",
    "for key in keys:\n",
    "    print(f\"Shape of {key}: {dataset[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMZwtPdMJ_yy"
   },
   "outputs": [],
   "source": [
    "# Standardization of dataset and conversion to int\n",
    "# WARNING: dictionary makes a mess, you must create a new data structure\n",
    "def standardization(images):\n",
    "    min_value = images.min()\n",
    "    max_value = images.max()\n",
    "\n",
    "    max_value = max_value - min_value\n",
    "\n",
    "    images = ((images - min_value)/max_value) * 255\n",
    "    return images.astype(int)\n",
    "\n",
    "# images = standardization(dataset['data'])\n",
    "images = dataset['data']\n",
    "\n",
    "# Creation of new arrays instead of dict\n",
    "labels_dict = {'healthy': 0, 'unhealthy': 1}\n",
    "labels = []   #target values\n",
    "for i in range(len(dataset['labels'])):\n",
    "  labels.append(labels_dict[tuple(dataset['labels'])[i]])  #adding targets to new array, using dictionary (using tuple because of the type of np array)\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hZOgSczumWp"
   },
   "outputs": [],
   "source": [
    "# Display a sample of images from the training-validation dataset\n",
    "num_img = 20\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(20,20))\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i in range(num_img):\n",
    "    # Select a random index\n",
    "    idx = np.random.randint(0, len(images))\n",
    "\n",
    "    ax = axes[i % num_img]\n",
    "    # Display the normalized image using imshow\n",
    "    ax.imshow(images[idx])\n",
    "    ax.set_title({labels[idx]})  # Show the corresponding digit label\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN7sMyAXIC2c"
   },
   "outputs": [],
   "source": [
    "# Assessment of dataset balance\n",
    "print(pd.DataFrame(labels, columns=['label'])['label'].value_counts())\n",
    "\n",
    "# Classes are unbalanced, healthy elements are many more than unhealthy ones; important to stratify the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_Nq988AQbUS"
   },
   "outputs": [],
   "source": [
    "# Split between training-validation and test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(images, labels, random_state=seed, test_size=0.1, stratify=labels)\n",
    "\n",
    "# Split between training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=seed, test_size=0.2, stratify=y_trainval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ycsq4pYOXcRb"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding for the classes\n",
    "\n",
    "y_train = tfk.utils.to_categorical(y_train)\n",
    "y_val = tfk.utils.to_categorical(y_val)\n",
    "y_test = tfk.utils.to_categorical(y_test)\n",
    "y_trainval = tfk.utils.to_categorical(y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhMR-3-TSFQ2"
   },
   "outputs": [],
   "source": [
    "# Print the shapes of the sets\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Training Label Shape:\", y_train.shape)\n",
    "print(\"Validation Data Shape:\", X_val.shape)\n",
    "print(\"Validation Label Shape:\", y_val.shape)\n",
    "print(\"Test Data Shape:\", X_test.shape)\n",
    "print(\"Test Label Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUgSbYq5WiIP"
   },
   "source": [
    "## Build the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T37cgG4R_X0"
   },
   "outputs": [],
   "source": [
    "# Define key model parameters\n",
    "input_shape = X_train.shape[1:]  # Input shape for the model\n",
    "output_shape = y_train.shape[1]  # Output shape for the model\n",
    "batch_size = 128                 # Batch size for training\n",
    "epochs = 200                     # Number of training epochs\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Output Shape:\", output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download and customize the pre-trained model"
   ],
   "metadata": {
    "id": "kGyvTz88UimN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_ConvNeXtLarge(input_shape, output_shape, lr, seed=seed):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    #Load the pre-trained ResNet-50 model (excluding the top classification layer)\n",
    "    base_model = ConvNeXtLarge(\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        )\n",
    "\n",
    "    # Freeze the weights of the pre-trained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "    # Add custom layers for your specific classification task\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = tfkl.Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    # add a regularisation layer\n",
    "    x = tfkl.Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # add a regularisation layer\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "    predictions = Dense(output_shape, activation='softmax')(x)  # Two units for binary classification\n",
    "\n",
    "    # Create the final model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions, name = 'ConvNeXtLarge_custom')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "GLgsSpfKUTzX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create support data structures\n"
   ],
   "metadata": {
    "id": "_Ol0jPnXZYp9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# create an array of 3 possible learning rates\n",
    "\n",
    "lrs = [0.001, 0.0001, 0.01] # the best was 0.001\n",
    "\n",
    "# create an array of 2 possible batch sizes, multiple of 2\n",
    "\n",
    "bss = [64, 32] # the best was 64\n",
    "\n",
    "# create an array of histories\n",
    "\n",
    "histories = []\n",
    "\n",
    "# create a list, each node must have: modelnam, accuracy and val_accuracy\n",
    "\n",
    "models_scores = []\n",
    "\n",
    "# create an array of model names\n",
    "\n",
    "model_names = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"] # this was an array of custom model names to save that will be filled later\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "ZJ2zUkTaZJVA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cycle through the support data structures to select the best hyperparameters"
   ],
   "metadata": {
    "id": "_6h5Yyn_ZfEW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    tfk.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tfk.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_accuracy\",\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tfk.callbacks.ModelCheckpoint(\n",
    "        filepath=\"ConvNeXtLarge.h5\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]"
   ],
   "metadata": {
    "id": "rHIOdMYlhFyu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# cycle through the learning rates\n",
    "i = 0;\n",
    "\n",
    "for lr in lrs:\n",
    "\n",
    "    # cycle though the batch sizes\n",
    "\n",
    "    for bs in bss:\n",
    "\n",
    "        ConvNeXtLarge_model = build_ConvNeXtLarge(input_shape, output_shape, lr)\n",
    "\n",
    "\n",
    "        model_names[i] = 'ConvNeXtLarge_lr_' + str(lr)+ '_bs_' + str(bs)\n",
    "\n",
    "        # Define the callbacks\n",
    "        callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_accuracy\",\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath=\"ConvNeXtLarge.h5\",\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        ]\n",
    "\n",
    "        # Fit the model\n",
    "        history = ConvNeXtLarge_model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            batch_size=bs,\n",
    "            epochs=200,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks\n",
    "        ).history\n",
    "\n",
    "\n",
    "         # How many initial epochs to skip in the plot\n",
    "        begin_plot = 2\n",
    "\n",
    "        # Find the epoch with the highest validation accuracy\n",
    "        best_epoch = np.argmax(history['val_accuracy'][begin_plot:])\n",
    "\n",
    "        # Save the trained model\n",
    "        ConvNeXtLarge_model.save(\"All_Colab_Scripts/ConvNeXtLarge/Cycle_Trained/\" + model_names[i] + \"_val_\" + str(history['val_accuracy'][best_epoch+begin_plot]))\n",
    "\n",
    "\n",
    "        # Plot training and validation performance metrics\n",
    "        plt.figure(figsize=(20, 5))\n",
    "\n",
    "        # Plot training and validation loss\n",
    "        plt.plot(history['loss'][begin_plot:], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "        plt.plot(history['val_loss'][begin_plot:], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Categorical Crossentropy')\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        plt.figure(figsize=(20, 5))\n",
    "\n",
    "        # Plot training and validation accuracy, highlighting the best epoch\n",
    "        plt.plot(history['accuracy'][begin_plot:], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "        plt.plot(history['val_accuracy'][begin_plot:], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "        plt.plot(best_epoch, history['val_accuracy'][best_epoch+begin_plot], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Accuracy')\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # add the model name, accuracy and val_accuracy to the list\n",
    "        models_scores.append([str(model_names[i]), history['accuracy'][best_epoch+begin_plot], history['val_accuracy'][best_epoch+begin_plot]])\n",
    "\n",
    "        del history\n",
    "\n",
    "        del ConvNeXtLarge_model\n",
    "\n",
    "        i = i + 1"
   ],
   "metadata": {
    "id": "2ke-eoSRY8YI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot the results"
   ],
   "metadata": {
    "id": "3wg4dzaFZMS7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dataframe with the list\n",
    "df = pd.DataFrame(models_scores, columns = ['model', 'accuracy', 'val_accuracy'])\n",
    "print(df)\n",
    "# plot the dataframe\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.barplot(x='model', y='accuracy', data=df, color='#ff7f0e', alpha=0.8)\n",
    "sns.barplot(x='model', y='val_accuracy', data=df, color='#4D61E2', alpha=0.8)\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "uB6RCjGuZKdA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine Tuning"
   ],
   "metadata": {
    "id": "a_PwyABv7mPM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ConvNeXtLarge_model = tfk.models.load_model('All_Colab_Scripts/ConvNeXtLarge/Cycle_Trained/ConvNeXtLarge_lr_0.001_bs_64_val_0.9717361927032471') # load the best model\n",
    "\n",
    "name = 'ConvNeXtLarge_FineTuned'\n",
    "\n",
    "ConvNeXtLarge_model.summary()"
   ],
   "metadata": {
    "id": "cGV73QSGa9qV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Enable layers\n",
    "trainable_layers = 15\n",
    "for i, layer in enumerate(ConvNeXtLarge_model.layers[(len(ConvNeXtLarge_model.layers) - trainable_layers):]):\n",
    "  layer.trainable=True # da 1 a N sono trainabili\n",
    "for i, layer in enumerate(ConvNeXtLarge_model.layers[:(len(ConvNeXtLarge_model.layers) - trainable_layers)]):\n",
    "  layer.trainable=False # da 1 a N non sono trainabili\n",
    "for i, layer in enumerate(ConvNeXtLarge_model.layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "ConvNeXtLarge_model.summary()"
   ],
   "metadata": {
    "id": "_5JvPYxeugO5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install keras-cv tensorflow --upgrade\n",
    "import keras_cv\n",
    "import keras_core as keras\n",
    "import numpy as np\n",
    "\n",
    "ConvNeXtLarge_model = tf.keras.Sequential([\n",
    "  # Add the preprocessing layers you created earlier.\n",
    "  #layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "  keras_cv.layers.RandomSaturation((0.0, 0.5)),\n",
    "  layers.RandomTranslation(0.2, 0.2),\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomRotation(0.15),\n",
    "  layers.RandomContrast(0.5),\n",
    "  layers.RandomBrightness(0.2),\n",
    "  layers.RandomZoom(.3, .3),\n",
    "  keras_cv.layers.RandomShear(.1, .1),\n",
    "  #keras_cv.layers.RandAugment(value_range=(0, 1), augmentations_per_image=3, magnitude=0.3),\n",
    "\n",
    "  # Rest of the model.\n",
    "  ConvNeXtLarge_model\n",
    "])\n",
    "\n"
   ],
   "metadata": {
    "id": "GTjLx_IrtYMz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ConvNeXtLarge_model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "id": "PqGHDY-U-yP4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Define early stopping callbacks\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    tfk.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tfk.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\",\n",
    "        factor=0.5,\n",
    "        patience=15,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = ConvNeXtLarge_model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "# Save the trained model\n",
    "ConvNeXtLarge_model.save(\"All_Colab_Scripts/ConvNeXtLarge/Fine_Tuned/Final_\" + name)\n",
    "\n",
    "# How many initial epochs to skip in the plot\n",
    "begin_plot = 2\n",
    "\n",
    "# Find the epoch with the highest validation accuracy\n",
    "best_epoch = np.argmax(history['val_accuracy'][begin_plot:])\n",
    "\n",
    "# Plot training and validation performance metrics\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history['loss'][begin_plot:], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_loss'][begin_plot:], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation accuracy, highlighting the best epoch\n",
    "plt.plot(history['accuracy'][begin_plot:], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_accuracy'][begin_plot:], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.plot(best_epoch, history['val_accuracy'][best_epoch+begin_plot], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del history\n",
    "\n",
    "del ConvNeXtLarge_model"
   ],
   "metadata": {
    "id": "gy-Qx9YSw5i_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make inference on the test set"
   ],
   "metadata": {
    "collapsed": false,
    "id": "7hKFUeit7BM4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "name = \"All_Colab_Scripts/ConvNeXtLarge/Fine_Tuned/Final_ConvNeXtLarge_fine\"\n",
    "\n",
    "ConvNeXtLarge_model = tfk.models.load_model(name)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = ConvNeXtLarge_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = ConvNeXtLarge_model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "\n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "NczmHmXr1Opg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Re-Train on validation and train"
   ],
   "metadata": {
    "id": "ZwWkrK4iLQ9M"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24XAqi6dghH2"
   },
   "outputs": [],
   "source": [
    "# Define key model parameters\n",
    "input_shape = X_trainval.shape[1:]  # Input shape for the model\n",
    "output_shape = y_trainval.shape[1]  # Output shape for the model\n",
    "batch_size = 64                 # Batch size for training\n",
    "epochs = 200                     # Number of training epochs\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Output Shape:\", output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "id": "Is2nn6BjgfSb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "name = \"All_Colab_Scripts/ConvNeXtLarge/Fine_Tuned/Final_ConvNeXtLarge_fine\"\n",
    "\n",
    "ConvNeXtLarge_model = tfk.models.load_model(name)\n",
    "\n",
    "ConvNeXtLarge_model.summary()"
   ],
   "metadata": {
    "id": "aBL3NKy6S5-I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Keep only the layer corresponding to the model without the augmentation layers\n",
    "\n",
    "print(str(ConvNeXtLarge_model.layers[8]))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(ConvNeXtLarge_model.layers[8])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "WJI8xdt3ZBkF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# unfreeze the layers\n",
    "\n",
    "print(model.layers[0].name)\n",
    "\n",
    "trainable_layers = 11\n",
    "for i, layer in enumerate(model.layers[0].layers[(len(model.layers[0].layers) - trainable_layers):]):\n",
    "  layer.trainable=True # da 1 a N sono trainabili\n",
    "for i, layer in enumerate(model.layers[0].layers[:(len(model.layers[0].layers) - trainable_layers)]):\n",
    "  layer.trainable=False # da 1 a N non sono trainabili\n",
    "for i, layer in enumerate(model.layers[0].layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "model.layers[0].summary()"
   ],
   "metadata": {
    "id": "ghgmfTes4F5b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    tfk.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tfk.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\",\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_trainval,\n",
    "    y_trainval,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.save(\"All_Colab_Scripts/ConvNeXtLarge/Final/FinalModel\")\n"
   ],
   "metadata": {
    "id": "mNyf--i6LUfK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Evaluation"
   ],
   "metadata": {
    "id": "0ITSy5fAS8fg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "\n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "5vtQBBjBkJX_"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
