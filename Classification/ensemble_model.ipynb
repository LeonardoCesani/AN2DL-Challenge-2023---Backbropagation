{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToY-4vBFTHpI"
   },
   "source": [
    "# Ensembling of ConvNeXtLarge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6efF90wTUto"
   },
   "source": [
    "# Base imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEYwzWQmqBh8"
   },
   "outputs": [],
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 90\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "\n",
    "!pip install keras-cv tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2970,
     "status": "ok",
     "timestamp": 1700215874418,
     "user": {
      "displayName": "matteo colella",
      "userId": "16573624712198557945"
     },
     "user_tz": -60
    },
    "id": "uhoK_GByqP1x",
    "outputId": "0f9697b7-19d9-4528-8de9-dfb8f9a45f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.applications import ConvNeXtLarge\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPool2D,  \\\n",
    "    Dropout, Dense, Input, concatenate,      \\\n",
    "    GlobalAveragePooling2D, AveragePooling2D,\\\n",
    "    Flatten\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7074,
     "status": "ok",
     "timestamp": 1700215881488,
     "user": {
      "displayName": "matteo colella",
      "userId": "16573624712198557945"
     },
     "user_tz": -60
    },
    "id": "ZP6GvRzKqVv4",
    "outputId": "52c6f027-e940-4e77-c17c-0b135567e808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import keras_cv\n",
    "import keras_core as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ8CEVmhLBwW"
   },
   "source": [
    "# Build the ensembled models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LwZacXITaEV"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_zqNtm08QZV"
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "train = np.load('Scripts/AugmentationScript/MergedDatasetV2_custom_parameters/train_dataset_aug_mix_1.npz', allow_pickle=True)\n",
    "validation = np.load('Scripts/AugmentationScript/MergedDatasetV2_custom_parameters/val_dataset.npz', allow_pickle=True)\n",
    "test = np.load('Scripts/AugmentationScript/MergedDatasetV2_custom_parameters/test_dataset.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOeNp8aK9F8q"
   },
   "outputs": [],
   "source": [
    "# Divite into data and labels\n",
    "X_train = train['data']\n",
    "y_train = train['labels']\n",
    "\n",
    "X_val = validation['data']\n",
    "y_val = validation['labels']\n",
    "\n",
    "X_test = test['data']\n",
    "y_test = test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0GQEfO4I0jd"
   },
   "outputs": [],
   "source": [
    "# Display a sample of images from the training-validation dataset\n",
    "num_img = 20\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(20,20))\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i in range(num_img):\n",
    "    # Select a random index\n",
    "    idx = np.random.randint(0, len(X_train))\n",
    "\n",
    "    ax = axes[i % num_img]\n",
    "    # Display the normalized image using imshow\n",
    "    ax.imshow(X_train[idx])\n",
    "    ax.set_title({y_train[idx]})  # Show the corresponding digit label\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KxP-jwG90db"
   },
   "outputs": [],
   "source": [
    "# Transformation of the labels from categorical to numerical\n",
    "\n",
    "labels_dict = {'healthy': 0, 'unhealthy': 1}\n",
    "\n",
    "y_train_array = []\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "  y_train_array.append(labels_dict[tuple(y_train)[i]])\n",
    "y_train = np.array(y_train_array)\n",
    "\n",
    "y_val_array = []\n",
    "for i in range(len(y_val)):\n",
    "  y_val_array.append(labels_dict[tuple(y_val)[i]])\n",
    "y_val = np.array(y_val_array)\n",
    "\n",
    "\n",
    "y_test_array = []\n",
    "for i in range(len(y_test)):\n",
    "  y_test_array.append(labels_dict[tuple(y_test)[i]])\n",
    "y_test = np.array(y_test_array)\n",
    "\n",
    "# One-hot transformation of labels\n",
    "y_train = tfk.utils.to_categorical(y_train)\n",
    "y_val = tfk.utils.to_categorical(y_val)\n",
    "y_test = tfk.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1700215910368,
     "user": {
      "displayName": "matteo colella",
      "userId": "16573624712198557945"
     },
     "user_tz": -60
    },
    "id": "OiMwaehM_dWf",
    "outputId": "971bda7a-d9aa-44b4-dfc8-4fc7b9dc0b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (8253, 96, 96, 3)\n",
      "Training Label Shape: (8253, 2)\n",
      "Validation Data Shape: (872, 96, 96, 3)\n",
      "Validation Label Shape: (872, 2)\n",
      "Test Data Shape: (484, 96, 96, 3)\n",
      "Test Label Shape: (484, 2)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the sets\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Training Label Shape:\", y_train.shape)\n",
    "print(\"Validation Data Shape:\", X_val.shape)\n",
    "print(\"Validation Label Shape:\", y_val.shape)\n",
    "print(\"Test Data Shape:\", X_test.shape)\n",
    "print(\"Test Label Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6IrRdfHMdcI"
   },
   "outputs": [],
   "source": [
    "del train\n",
    "del validation\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUgSbYq5WiIP"
   },
   "source": [
    "## Build the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1700215910368,
     "user": {
      "displayName": "matteo colella",
      "userId": "16573624712198557945"
     },
     "user_tz": -60
    },
    "id": "6T37cgG4R_X0",
    "outputId": "0a9609b6-9454-4775-c097-b95cf0302840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Input Shape: (96, 96, 3)\n",
      "Output Shape: 2\n"
     ]
    }
   ],
   "source": [
    "# Define key model parameters\n",
    "input_shape = X_train.shape[1:]  # Input shape for the model\n",
    "output_shape = y_train.shape[1]  # Output shape for the model\n",
    "batch_size = 128                 # Batch size for training\n",
    "epochs = 200                     # Number of training epochs\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Output Shape:\", output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JYYjIn5PTgJk"
   },
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzvBhizcTgJk"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "# Disable scientific notation for clarity\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Load the model\n",
    "keras_model = tensorflow.keras.models.load_model('Deliverables/SubmissionModels/model_1', compile=False)\n",
    "keras_model._name = 'model1'\n",
    "keras_model2 = tensorflow.keras.models.load_model('Deliverables/SubmissionModels/model_2', compile=False)\n",
    "keras_model2._name = 'model2'\n",
    "models = [keras_model, keras_model2]\n",
    "#model_input = tf.keras.Input(shape=(125, 125, 3))\n",
    "model_input = tf.keras.Input(shape=(96, 96, 3))\n",
    "model_outputs = [model(model_input) for model in models]\n",
    "\n",
    "\n",
    "ensemble_output = layers.average(model_outputs)\n",
    "ensemble_model = tf.keras.Model(inputs=model_input, outputs=ensemble_output)\n",
    "\n",
    "ensemble_model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1700208703291,
     "user": {
      "displayName": "matteo colella",
      "userId": "16573624712198557945"
     },
     "user_tz": -60
    },
    "id": "FkQ4sYXBT7oN",
    "outputId": "a61a4836-34af-43ab-e970-0b60e1e5584d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 96, 96, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " model1 (Sequential)         (None, 2)                    1966574   ['input_2[0][0]']             \n",
      "                                                          74                                      \n",
      "                                                                                                  \n",
      " model2 (Functional)         (None, 2)                    3933149   ['input_2[0][0]']             \n",
      "                                                          48                                      \n",
      "                                                                                                  \n",
      " average (Average)           (None, 2)                    0         ['model1[0][0]',              \n",
      "                                                                     'model2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 589972422 (2.20 GB)\n",
      "Trainable params: 39064710 (149.02 MB)\n",
      "Non-trainable params: 550907712 (2.05 GB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ensemble_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFx0yYndTgJl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "score = ensemble_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = ensemble_model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "\n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1c-uVV2TgJl"
   },
   "outputs": [],
   "source": [
    "ensemble_model.save(\"All_Colab_Scripts/ConvNeXtLarge/Ensembled/Ensembled_FinalModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1-rcVwKTgJl"
   },
   "outputs": [],
   "source": [
    "tfk.utils.plot_model(ensemble_model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rFIpMsyqTgJl"
   },
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kShGa3RtTgJl"
   },
   "outputs": [],
   "source": [
    "ConvNeXtLarge_model = tfk.models.load_model('All_Colab_Scripts/ConvNeXtLarge/Ensembled/Ensembled_FinalModel')\n",
    "\n",
    "\n",
    "name = 'Ensembled_FinalModel'\n",
    "\n",
    "ConvNeXtLarge_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5JvPYxeugO5"
   },
   "outputs": [],
   "source": [
    "# Enable one of the two models ensembled to perform fine tuning\n",
    "trainable_layers = 15\n",
    "for i, layer in enumerate(ConvNeXtLarge_model.layers[1].layers[(len(ConvNeXtLarge_model.layers[1].layers) - trainable_layers):]):\n",
    "  layer.trainable=True # da 1 a N sono trainabili\n",
    "for i, layer in enumerate(ConvNeXtLarge_model.layers[1].layers[:(len(ConvNeXtLarge_model.layers[1].layers) - trainable_layers)]):\n",
    "  layer.trainable=False # da 1 a N non sono trainabili\n",
    "for i, layer in enumerate(ConvNeXtLarge_model.layers[1].layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "# Print the model summary\n",
    "ConvNeXtLarge_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTjLx_IrtYMz"
   },
   "outputs": [],
   "source": [
    "!pip install keras-cv tensorflow --upgrade\n",
    "import keras_cv\n",
    "import keras_core as keras\n",
    "import numpy as np\n",
    "\n",
    "ConvNeXtLarge_model = tf.keras.Sequential([\n",
    "  # Add the preprocessing layers you created earlier.\n",
    "  #layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "  #keras_cv.layers.RandomSaturation((0.0, 0.5)),\n",
    "  #layers.RandomTranslation(0.2, 0.2),\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomRotation((0.3, 0.5)),\n",
    "  layers.RandomContrast((0.0, 0.3)),\n",
    "  layers.RandomBrightness((0.0, 0.3)),\n",
    "  layers.RandomZoom((-0.14, 0.14), (-0.14, 0.14)),\n",
    "  #keras_cv.layers.RandomShear(.1, .1),\n",
    "  #keras_cv.layers.RandAugment(value_range=(0, 1), augmentations_per_image=3, magnitude=0.3),\n",
    "\n",
    "  # Rest of the model.\n",
    "  ConvNeXtLarge_model\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqGHDY-U-yP4"
   },
   "outputs": [],
   "source": [
    "ConvNeXtLarge_model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy-Qx9YSw5i_"
   },
   "outputs": [],
   "source": [
    "# train the model on augmented data, we choose to monitor only loss since the validtion set is not augmented and the model must improve only on augmented data in this phase\n",
    "# Define early stopping callbacks\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    tfk.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tfk.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\",\n",
    "        factor=0.5,\n",
    "        patience=15,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = ConvNeXtLarge_model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "# Save the trained model\n",
    "ConvNeXtLarge_model.save(\"All_Colab_Scripts/ConvNeXtLarge/Fine_Tuned/ft_\" + name)\n",
    "\n",
    "# How many initial epochs to skip in the plot\n",
    "begin_plot = 2\n",
    "\n",
    "# Find the epoch with the highest validation accuracy\n",
    "best_epoch = np.argmax(history['val_accuracy'][begin_plot:])\n",
    "\n",
    "# Plot training and validation performance metrics\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history['loss'][begin_plot:], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_loss'][begin_plot:], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation accuracy, highlighting the best epoch\n",
    "plt.plot(history['accuracy'][begin_plot:], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_accuracy'][begin_plot:], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.plot(best_epoch, history['val_accuracy'][best_epoch+begin_plot], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del history\n",
    "\n",
    "del ConvNeXtLarge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NczmHmXr1Opg"
   },
   "outputs": [],
   "source": [
    "name = \"All_Colab_Scripts/ConvNeXtLarge/Fine_Tuned/ft_Ensembled_FinalModel\"\n",
    "\n",
    "ConvNeXtLarge_model = tfk.models.load_model(name)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = ConvNeXtLarge_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = ConvNeXtLarge_model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "\n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwWkrK4iLQ9M"
   },
   "source": [
    "# Re-Train on validation and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFZW15-FghH2"
   },
   "source": [
    "## Build the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFIu_ZtbiWoV"
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "train = np.load('Scripts/AugmentationScript/MergedDatasetV2_custom_parameters/train_val_dataset.npz', allow_pickle=True)\n",
    "\n",
    "X_trainval = train['data']\n",
    "y_trainval = train['labels']\n",
    "\n",
    "labels_dict = {'healthy': 0, 'unhealthy': 1}\n",
    "\n",
    "y_trainval_array = []\n",
    "\n",
    "for i in range(len(y_trainval)):\n",
    "  y_trainval_array.append(labels_dict[tuple(y_trainval)[i]])\n",
    "y_trainval = np.array(y_trainval_array)\n",
    "\n",
    "y_trainval = tfk.utils.to_categorical(y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24XAqi6dghH2"
   },
   "outputs": [],
   "source": [
    "# Define key model parameters\n",
    "input_shape = X_trainval.shape[1:]  # Input shape for the model\n",
    "output_shape = y_trainval.shape[1]  # Output shape for the model\n",
    "batch_size = 64                 # Batch size for training\n",
    "epochs = 200                     # Number of training epochs\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Output Shape:\", output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is2nn6BjgfSb"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBL3NKy6S5-I"
   },
   "outputs": [],
   "source": [
    "name = \"All_Colab_Scripts/ConvNeXtLarge/Fine_Tuned/ft_Ensembled_FinalModel\"\n",
    "\n",
    "EfficientNetV2S_model = tfk.models.load_model(name)\n",
    "\n",
    "EfficientNetV2S_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJI8xdt3ZBkF"
   },
   "outputs": [],
   "source": [
    "\n",
    "# V1\n",
    "\n",
    "print(str(EfficientNetV2S_model.layers[5]))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(EfficientNetV2S_model.layers[5])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yypQsrc20pQm"
   },
   "outputs": [],
   "source": [
    "model1 = model.layers[0].layers[1]\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model2 = model.layers[0].layers[2]\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dNpiSXq04TX"
   },
   "outputs": [],
   "source": [
    "\n",
    "trainable_layers = 15\n",
    "for i, layer in enumerate(model1.layers[0].layers[(len(model1.layers[0].layers) - trainable_layers):]):\n",
    "  layer.trainable=True # da 1 a N sono trainabili\n",
    "for i, layer in enumerate(model1.layers[0].layers[:(len(model1.layers[0].layers) - trainable_layers)]):\n",
    "  layer.trainable=False # da 1 a N non sono trainabili\n",
    "for i, layer in enumerate(model1.layers[0].layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "model1.layers[0].summary()\n",
    "\n",
    "trainable_layers = 15\n",
    "for i, layer in enumerate(model2.layers[0].layers[(len(model2.layers[0].layers) - trainable_layers):]):\n",
    "  layer.trainable=True # da 1 a N sono trainabili\n",
    "for i, layer in enumerate(model2.layers[0].layers[:(len(model2.layers[0].layers) - trainable_layers)]):\n",
    "  layer.trainable=False # da 1 a N non sono trainabili\n",
    "for i, layer in enumerate(model2.layers[0].layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "model2.layers[0].summary()\n",
    "\n",
    "model1.layers[0]._name = \"ConvNeXtLarge_ft\"\n",
    "\n",
    "model2.layers[0]._name = \"ConvNeXtLarge_cole\"\n",
    "\n",
    "models = [model1.layers[0], model2.layers[0]]\n",
    "#model_input = tf.keras.Input(shape=(125, 125, 3))\n",
    "model_input = tf.keras.Input(shape=(96, 96, 3))\n",
    "model_outputs = [model(model_input) for model in models]\n",
    "\n",
    "\n",
    "ensemble_output = layers.average(model_outputs)\n",
    "ensemble_model = tf.keras.Model(inputs=model_input, outputs=ensemble_output, name = \"ensembled\")\n",
    "\n",
    "ensemble_model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghgmfTes4F5b"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.layers[0].name)\n",
    "\n",
    "trainable_layers = 20\n",
    "for i, layer in enumerate(model.layers[0].layers[2].layers[(len(model.layers[0].layers[2].layers) - trainable_layers):]):\n",
    "  layer.trainable=True # da 1 a N sono trainabili\n",
    "for i, layer in enumerate(model.layers[0].layers[2].layers[:(len(model.layers[0].layers[2].layers) - trainable_layers)]):\n",
    "  layer.trainable=False # da 1 a N non sono trainabili\n",
    "for i, layer in enumerate(model.layers[0].layers[2].layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "model.layers[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utEYwWERhat6"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNyf--i6LUfK"
   },
   "outputs": [],
   "source": [
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    tfk.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tfk.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\",\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tfk.callbacks.ModelCheckpoint(\n",
    "        filepath=\"EfficientNetV2S_copilot/V2S_copilot_3.h5\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = ensemble_model.fit(\n",
    "    X_trainval,\n",
    "    y_trainval,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "ensemble_model.save(\"All_Colab_Scripts/ConvNeXtLarge/Final/Ens_Leo_FinalModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCEpdA0L5YzL"
   },
   "outputs": [],
   "source": [
    "ensemble_model.save(\"All_Colab_Scripts/ConvNeXtLarge/Final/Ens_Leo_FinalModel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ITSy5fAS8fg"
   },
   "source": [
    "# Improve futher the ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJpe3M5laHD3"
   },
   "source": [
    "### Import ensembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6yjWxxCaKnY"
   },
   "outputs": [],
   "source": [
    "ConvNeXtLarge_model = tfk.models.load_model('Deliverables/SubmissionModels/ConvNeXtLarge_Ensembled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7MYAyE5acD6"
   },
   "outputs": [],
   "source": [
    "ConvNeXtLarge_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_B6EKopat9z"
   },
   "source": [
    "### Extract the ensembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNJ1IWkYawaA"
   },
   "outputs": [],
   "source": [
    "ens_model = ConvNeXtLarge_model.layers[5]\n",
    "ens_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-W5B0bKbEhJ"
   },
   "source": [
    "### Divide the models and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxK0RTXxatnd"
   },
   "outputs": [],
   "source": [
    "model1 = ens_model.layers[1]\n",
    "\n",
    "model2 = ens_model.layers[2]\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-acV8TgbvoM"
   },
   "outputs": [],
   "source": [
    "# freeze layers of first model\n",
    "trainable_layers = 0\n",
    "for i, layer in enumerate(model1.layers[0].layers[(len(model1.layers[0].layers) - trainable_layers):]):\n",
    "  layer.trainable=True # da 1 a N sono trainabili\n",
    "for i, layer in enumerate(model1.layers[0].layers[:(len(model1.layers[0].layers) - trainable_layers)]):\n",
    "  layer.trainable=False # da 1 a N non sono trainabili\n",
    "for i, layer in enumerate(model1.layers[0].layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "\n",
    "model1.layers[0].summary()\n",
    "\n",
    "\n",
    "# unfreeze layers of second model\n",
    "trainable_layers = 15\n",
    "for i, layer in enumerate(model2.layers[0].layers[(len(model2.layers[0].layers) - trainable_layers):]):\n",
    "  layer.trainable=True # da 1 a N sono trainabili\n",
    "for i, layer in enumerate(model2.layers[0].layers[:(len(model2.layers[0].layers) - trainable_layers)]):\n",
    "  layer.trainable=False # da 1 a N non sono trainabili\n",
    "for i, layer in enumerate(model2.layers[0].layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "model2.layers[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3k7Hb7Icvgk"
   },
   "outputs": [],
   "source": [
    "# merge the layers\n",
    "# Disable scientific notation for clarity\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Load the model\n",
    "keras_model = model1.layers[0]\n",
    "keras_model._name = 'NonClasswise'\n",
    "keras_model2 = model2.layers[0]\n",
    "keras_model2._name = 'Classwise'\n",
    "models = [keras_model, keras_model2]\n",
    "#model_input = tf.keras.Input(shape=(125, 125, 3))\n",
    "model_input = tf.keras.Input(shape=(96, 96, 3))\n",
    "model_outputs = [model(model_input) for model in models]\n",
    "\n",
    "\n",
    "ensemble_output = layers.average(model_outputs)\n",
    "ensemble_model = tf.keras.Model(inputs=model_input, outputs=ensemble_output)\n",
    "\n",
    "ensemble_model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "ensemble_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxPs5MW8cjtF"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "  # Add the preprocessing layers you created earlier.\n",
    "  #layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "  keras_cv.layers.RandomSaturation((0.0, 0.5)),\n",
    "  layers.RandomTranslation(0.2, 0.2),\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomRotation((0.3, 0.5)),\n",
    "  layers.RandomContrast((0.0, 0.3)),\n",
    "  layers.RandomBrightness((0.0, 0.3)),\n",
    "  layers.RandomZoom((-0.14, 0.14), (-0.14, 0.14)),\n",
    "  keras_cv.layers.RandomShear(.1, .1),\n",
    "  #keras_cv.layers.RandAugment(value_range=(0, 1), augmentations_per_image=3, magnitude=0.3),\n",
    "\n",
    "  # Rest of the model.\n",
    "  ensemble_model\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_k1uW3ctgIv4"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tfk.optimizers.AdamW(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model = tfk.models.load_model('Deliverables/SubmissionModels/ConvNeXtLarge_Classwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lVU6rE2MdwCx"
   },
   "outputs": [],
   "source": [
    "# train the model on augmented data, we choose to monitor only loss since the validtion set is not augmented and the model must improve only on augmented data in this phase\n",
    "# Define early stopping callbacks\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    tfk.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tfk.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\",\n",
    "        factor=0.5,\n",
    "        patience=15,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"All_Colab_Scripts/ConvNeXtLarge/Fine_Tuned/FT2_ENSEMBLED\")\n",
    "\n",
    "# How many initial epochs to skip in the plot\n",
    "begin_plot = 2\n",
    "\n",
    "# Find the epoch with the highest validation accuracy\n",
    "best_epoch = np.argmax(history['val_accuracy'][begin_plot:])\n",
    "\n",
    "# Plot training and validation performance metrics\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history['loss'][begin_plot:], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_loss'][begin_plot:], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation accuracy, highlighting the best epoch\n",
    "plt.plot(history['accuracy'][begin_plot:], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_accuracy'][begin_plot:], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.plot(best_epoch, history['val_accuracy'][best_epoch+begin_plot], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del history\n",
    "\n",
    "#del ConvNeXtLarge_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "id": "YQ1gRe28e9g6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44bkuaYteQg1"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "\n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "E6efF90wTUto",
    "eJ8CEVmhLBwW",
    "5LwZacXITaEV",
    "WUgSbYq5WiIP",
    "JYYjIn5PTgJk",
    "rFIpMsyqTgJl",
    "ZwWkrK4iLQ9M",
    "0ITSy5fAS8fg",
    "TJpe3M5laHD3",
    "v_B6EKopat9z",
    "I-W5B0bKbEhJ",
    "YQ1gRe28e9g6"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
