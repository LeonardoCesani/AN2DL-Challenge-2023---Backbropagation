{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap3o8JayfgEM"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR-w0eZ-ABIT"
   },
   "outputs": [],
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e06wPZJbADcv"
   },
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fN6wKImkADfL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a49TrZ82D-Fl"
   },
   "source": [
    "# Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB1vxLkYyfFQ"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data/train218.csv')\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsmfjXvZTZ70"
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFZ-h-7IBS5l"
   },
   "outputs": [],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXpSLGgC902P"
   },
   "outputs": [],
   "source": [
    "def plot_time_series(dataset, rows):\n",
    "    plt.figure(figsize=(17,5))\n",
    "    for i in range(rows):\n",
    "        plt.plot(dataset.iloc[i])\n",
    "    plt.title('Time Series')\n",
    "    plt.show()\n",
    "\n",
    "plot_time_series(dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bMTOOE0NDA9"
   },
   "outputs": [],
   "source": [
    "labels_size = 9\n",
    "X_train = dataset.iloc[:,:-labels_size]\n",
    "y_train = dataset.iloc[:,-labels_size:]\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AASFwzzCCGNn"
   },
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "output_shape = y_train.shape[1:]\n",
    "batch_size = 64\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7daWwqPXFAA"
   },
   "outputs": [],
   "source": [
    "print(input_shape)\n",
    "print(output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUqW6PgmNsem"
   },
   "source": [
    "Sequential Train-Test split and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhXIPG9uVQKz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.plot(X_train.T[0], label='Train')\n",
    "plt.plot(y_train.T[0], label='Test')\n",
    "plt.title('Train-Test Split')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jU3_acq542V-"
   },
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis = -1)\n",
    "y_train = np.expand_dims(y_train, axis = -1)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Categories"
   ],
   "metadata": {
    "id": "a9onoUln3b3J"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTAa-LJ4bvRV"
   },
   "outputs": [],
   "source": [
    "# Import the classes\n",
    "classes_train = pd.read_csv('Data/classes_train218.csv')\n",
    "classes_test = pd.read_csv('Data/classes_test218.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BajkEQxhfNHa"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "classes_dict = {\n",
    "    'A' : 0,\n",
    "    'B' : 1,\n",
    "    'C' : 2,\n",
    "    'D' : 3,\n",
    "    'E' : 4,\n",
    "    'F' : 5\n",
    "}\n",
    "\n",
    "classes_train = classes_train.applymap(classes_dict.get).values\n",
    "classes_test = classes_test.applymap(classes_dict.get).values\n",
    "# Convert numerical values to one-hot encoded vectors\n",
    "classes_train = to_categorical(classes_train, num_classes=6)\n",
    "classes_test = to_categorical(classes_test, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test data"
   ],
   "metadata": {
    "id": "EfZKbhxu3Vz5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtxEjJ24Um_K"
   },
   "outputs": [],
   "source": [
    "test_ds = pd.read_csv('Data/test218.csv')\n",
    "print(test_ds.shape)\n",
    "test_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIbmEZTbUpS2"
   },
   "outputs": [],
   "source": [
    "test_size = 9\n",
    "X_test = test_ds.iloc[:,:-test_size]\n",
    "y_test = test_ds.iloc[:,-test_size:]\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnkG9EqzUtg6"
   },
   "outputs": [],
   "source": [
    "X_test = np.expand_dims(X_test, axis = -1)\n",
    "y_test = np.expand_dims(y_test, axis = -1)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Best models\n"
   ],
   "metadata": {
    "id": "5TRHi1LF2sqF"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhuy0CsjSSns"
   },
   "source": [
    "# DA-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gn9VV6uISTlm"
   },
   "outputs": [],
   "source": [
    "def da_rnn(input_shape, output_shape, lstm_units=128, dense_units=64):\n",
    "    # Encoder\n",
    "    encoder_inputs = tfkl.Input(shape=input_shape)\n",
    "    encoder_lstm = tfkl.LSTM(units=lstm_units, return_sequences=True)(encoder_inputs)\n",
    "\n",
    "    # First Stage Attention\n",
    "    attention_1 = tfkl.Dense(input_shape[0], activation='softmax')(encoder_lstm)\n",
    "    attention_1 = tfkl.Permute((2, 1))(attention_1)\n",
    "    attention_1 = tfkl.Dot(axes=(2, 1))([attention_1, encoder_lstm])\n",
    "\n",
    "    # Decoder\n",
    "    repeat_vector = tfkl.RepeatVector(input_shape[0])(encoder_lstm[:, -1, :])\n",
    "    decoder_inputs = tfkl.Concatenate(axis=-1)([attention_1, repeat_vector])\n",
    "    decoder_lstm = tfkl.LSTM(units=lstm_units, return_sequences=True)(decoder_inputs)\n",
    "\n",
    "    # Second Stage Attention\n",
    "    attention_2 = tfkl.Dense(dense_units, activation='softmax')(decoder_lstm)\n",
    "    attention_2 = tfkl.Permute((2, 1))(attention_2)\n",
    "    attention_2 = tfkl.Dot(axes=(2, 1))([attention_2, decoder_lstm])\n",
    "\n",
    "    flatten = tfkl.Flatten()(attention_2)\n",
    "\n",
    "    # Output layer\n",
    "    output = tfkl.Dense(output_shape[0])(flatten)\n",
    "    output = tfkl.Activation('linear')(output)\n",
    "\n",
    "    model = tfk.Model(inputs=encoder_inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0YG1kWbW4nh"
   },
   "outputs": [],
   "source": [
    "model = da_rnn(input_shape, output_shape, 128, 128)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFmzB33WYHFx"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/Model_DARNN',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.TensorBoard(\n",
    "            log_dir='logs/Model_full',\n",
    "            histogram_freq=0,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "            update_freq='epoch',\n",
    "            profile_batch=2,\n",
    "            embeddings_freq=0,\n",
    "            embeddings_metadata=None\n",
    "        )\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMNnyVQpZTTq"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2yiSDC9go9f"
   },
   "outputs": [],
   "source": [
    "# Predict the test set using the model\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = tfk.metrics.mean_squared_error(y_test[:9, ].flatten(), predictions[:9, ].flatten()).numpy()\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test[:9, ].flatten(), predictions[:9, ].flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqnDQfUBbfZa"
   },
   "source": [
    "# DA-RNN with Class information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN2inDI6bk5P"
   },
   "outputs": [],
   "source": [
    "def da_rnn_with_classes(input_shape, output_shape, lstm_units_encoder=64, lstm_units_decoder=64, dense_units=64, num_classes=6):\n",
    "    # Encoder\n",
    "    encoder_inputs = tfk.layers.Input(shape=input_shape)\n",
    "    encoder_lstm = tfk.layers.LSTM(units=lstm_units_encoder, return_sequences=True)(encoder_inputs)\n",
    "\n",
    "    # Class information branch\n",
    "    class_inputs = tfk.layers.Input(shape=(num_classes,))\n",
    "    repeat_class = tfk.layers.RepeatVector(input_shape[0])(class_inputs)\n",
    "    class_combined = tfk.layers.Concatenate(axis=-1)([encoder_lstm, repeat_class])\n",
    "    encoder_lstm_with_class = tfk.layers.LSTM(units=lstm_units_decoder, return_sequences=True)(class_combined)\n",
    "\n",
    "    # First Stage Attention\n",
    "    attention_1 = tfk.layers.Dense(input_shape[0], activation='softmax')(encoder_lstm_with_class)\n",
    "    attention_1 = tfk.layers.Permute((2, 1))(attention_1)\n",
    "    attention_1 = tfk.layers.Dot(axes=(2, 1))([attention_1, encoder_lstm_with_class])\n",
    "\n",
    "    # Decoder\n",
    "    repeat_vector = tfk.layers.RepeatVector(input_shape[0])(encoder_lstm_with_class[:, -1, :])\n",
    "    decoder_inputs = tfk.layers.Concatenate(axis=-1)([attention_1, repeat_vector])\n",
    "    decoder_lstm = tfk.layers.LSTM(units=lstm_units_decoder, return_sequences=True)(decoder_inputs)\n",
    "\n",
    "    # Second Stage Attention\n",
    "    attention_2 = tfk.layers.Dense(input_shape[0], activation='softmax')(decoder_lstm)\n",
    "    attention_2 = tfk.layers.Permute((2, 1))(attention_2)\n",
    "    attention_2 = tfk.layers.Dot(axes=(2, 1))([attention_2, decoder_lstm])\n",
    "\n",
    "    flatten = tfk.layers.Flatten()(attention_2)\n",
    "\n",
    "    # Output layer\n",
    "    output = tfk.layers.Dense(output_shape[0])(flatten)\n",
    "    output = tfk.layers.Activation('linear')(output)\n",
    "\n",
    "\n",
    "    model = tfk.models.Model(inputs=[encoder_inputs, class_inputs], outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p35A03rfZ2sZ"
   },
   "outputs": [],
   "source": [
    "model = da_rnn_with_classes(input_shape, output_shape, 128, 128)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rbrUrQPZaAT8"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = [X_train, classes_train],\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/Model_DARNN_Class',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FoptMLrhkeDO"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISsgxH0Df61P"
   },
   "source": [
    "##Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvSG2bn2kKtz"
   },
   "outputs": [],
   "source": [
    "model = tfk.models.load_model('Models/Model_DARNN_Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1alm0G4gCZs"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Predict the test set using the model\n",
    "predictions = model.predict([X_test, classes_test], verbose=0)\n",
    "\n",
    "for pred in predictions:\n",
    "  for i in range(len(pred)):\n",
    "    if np.isnan(pred[i]):\n",
    "      pred[i] = 0\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = tfk.metrics.mean_squared_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")\n",
    "\n",
    "# Calculate and print Root Mean Squared Error (RMSE)\n",
    "root_mean_squared_error = tf.math.sqrt(mean_squared_error)\n",
    "print(f\"Root Mean Squared Error: {root_mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Percentage Error (MAPE)\n",
    "mean_absolute_percentage_error = tfk.metrics.mean_absolute_percentage_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error}\")\n",
    "\n",
    "# Calculate and print Coefficient of Determination (R^2)\n",
    "r2 = r2_score(y_test.flatten(), predictions.flatten())\n",
    "print(f\"Coefficient of Determination (R^2): {r2}\")\n",
    "\n",
    "print('residuals: ', np.square(predictions - y_test))\n",
    "\n",
    "# Calculate and print Adjusted Coefficient of Determination (Adj. R^2)\n",
    "adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print(f\"Adjusted Coefficient of Determination (Adj. R^2): {adjusted_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHaglE3Ghqnu"
   },
   "source": [
    "# DA-RNN with Advanced Attention Mechanism (SE) and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Exo4AkjkhvHv"
   },
   "outputs": [],
   "source": [
    "def se_block(in_block, ch, idx, ratio=16):\n",
    "    # 1. Define the squeeze block (global pooling)\n",
    "    squeeze = tfkl.GlobalAveragePooling1D(name='squeeze' + str(idx))(in_block)\n",
    "    # 2. Define the excitation block (ch/ratio to compress the information)\n",
    "    excitation = tfkl.Dense(units = ch/ratio, activation = 'relu', name='excitation_a' + str(idx))(squeeze)\n",
    "    # 3. Expand the information and mormalize with a sigmoid (give more information to some channel)\n",
    "    excitation = tfkl.Dense(units = ch, activation='sigmoid', name='excitation_b' + str(idx))(excitation)\n",
    "    # 4. Multiplication layer.\n",
    "    scaled_input = tfkl.multiply([in_block, excitation])\n",
    "\n",
    "    return scaled_input\n",
    "\n",
    "def da_rnn_se_classes(input_shape, output_shape, lstm_units_encoder=64, lstm_units_decoder=64, dense_units=64, num_classes=6):\n",
    "    # Encoder\n",
    "    encoder_inputs = tfk.layers.Input(shape=input_shape)\n",
    "    encoder_lstm = tfk.layers.LSTM(units=lstm_units_encoder, return_sequences=True)(encoder_inputs)\n",
    "\n",
    "    # Class information branch\n",
    "    class_inputs = tfk.layers.Input(shape=(num_classes,))\n",
    "    repeat_class = tfk.layers.RepeatVector(input_shape[0])(class_inputs)\n",
    "    class_combined = tfk.layers.Concatenate(axis=-1)([encoder_lstm, repeat_class])\n",
    "    encoder_lstm_with_class = tfk.layers.LSTM(units=lstm_units_decoder, return_sequences=True)(class_combined)\n",
    "\n",
    "    # First Stage Attention\n",
    "    attention_1 = se_block(encoder_lstm_with_class, lstm_units_decoder, idx = 1)\n",
    "\n",
    "    # Decoder\n",
    "    repeat_vector = tfk.layers.RepeatVector(input_shape[0])(encoder_lstm_with_class[:, -1, :])\n",
    "    decoder_inputs = tfk.layers.Concatenate(axis=-1)([attention_1, repeat_vector])\n",
    "    decoder_lstm = tfk.layers.LSTM(units=lstm_units_decoder, return_sequences=True)(decoder_inputs)\n",
    "\n",
    "    # Second Stage Attention\n",
    "    attention_2 = se_block(decoder_lstm, lstm_units_decoder, idx = 2)\n",
    "\n",
    "    flatten = tfk.layers.Flatten()(attention_2)\n",
    "\n",
    "    # Output layer\n",
    "    output = tfk.layers.Dense(output_shape[0])(flatten)\n",
    "    output = tfk.layers.Activation('linear')(output)\n",
    "\n",
    "\n",
    "    model = tfk.models.Model(inputs=[encoder_inputs, class_inputs], outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dZYFjBli0fV"
   },
   "outputs": [],
   "source": [
    "model = da_rnn_se_classes(input_shape, output_shape, 128, 64, 64)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SYRgiKwi0fW"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = [X_train, classes_train],\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/SubmissionModel_49',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14Iew0GxF5FW"
   },
   "outputs": [],
   "source": [
    "model.save('Models/SubmissionModel_49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R4ABR73i0fX"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2GhprIFoIOT"
   },
   "outputs": [],
   "source": [
    "model = tfk.models.load_model('Models/SubmissionModel_49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzHseRHSoIOU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Predict the test set using the model\n",
    "predictions = model.predict([X_test, classes_test], verbose=0)\n",
    "\n",
    "for pred in predictions:\n",
    "  for i in range(len(pred)):\n",
    "    if np.isnan(pred[i]):\n",
    "      pred[i] = 0\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = tfk.metrics.mean_squared_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")\n",
    "\n",
    "# Calculate and print Root Mean Squared Error (RMSE)\n",
    "root_mean_squared_error = tf.math.sqrt(mean_squared_error)\n",
    "print(f\"Root Mean Squared Error: {root_mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Percentage Error (MAPE)\n",
    "mean_absolute_percentage_error = tfk.metrics.mean_absolute_percentage_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error}\")\n",
    "\n",
    "# Calculate and print Coefficient of Determination (R^2)\n",
    "r2 = r2_score(y_test.flatten(), predictions.flatten())\n",
    "print(f\"Coefficient of Determination (R^2): {r2}\")\n",
    "\n",
    "# Calculate and print Adjusted Coefficient of Determination (Adj. R^2)\n",
    "adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print(f\"Adjusted Coefficient of Determination (Adj. R^2): {adjusted_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGlHVnkTOi1A"
   },
   "source": [
    "# DA-RNN with Advanced Attention Mechanism (CBAM) and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EaSuHXkOi1C"
   },
   "outputs": [],
   "source": [
    "def CBAM_block(in_block, ch, ratio=16):\n",
    "    # 1. Channel attention block\n",
    "    avg_pool = tf.reduce_mean(in_block, axis=1, keepdims=True)\n",
    "    max_pool = tf.reduce_max(in_block, axis=1, keepdims=True)\n",
    "\n",
    "    dense1 = tfkl.Dense(units=ch // ratio, activation='relu')\n",
    "    avg_reduced = dense1(avg_pool)\n",
    "    max_reduced = dense1(max_pool)\n",
    "\n",
    "    dense2 = tfkl.Dense(units=ch, activation='sigmoid')\n",
    "    avg_attention = dense2(avg_reduced)\n",
    "    max_attention = dense2(max_reduced)\n",
    "\n",
    "    x = tf.add(avg_attention, max_attention)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "    x = tf.multiply(in_block, x)\n",
    "\n",
    "    # 2. Spatial attention block\n",
    "    y_mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    y_max = tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "\n",
    "    y = tf.concat([y_mean, y_max], axis=-1)\n",
    "    y = tfkl.Conv1D(filters=1, kernel_size=5, padding='same', activation='sigmoid')(y)\n",
    "    y = tf.multiply(x, y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def da_rnn_CBAM_classes(input_shape, output_shape, lstm_units_encoder=64, lstm_units_decoder=64, dense_units=64, num_classes=6):\n",
    "    # Encoder\n",
    "    encoder_inputs = tfk.layers.Input(shape=input_shape)\n",
    "    encoder_lstm = tfk.layers.LSTM(units=lstm_units_encoder, return_sequences=True)(encoder_inputs)\n",
    "\n",
    "    # Class information branch\n",
    "    class_inputs = tfk.layers.Input(shape=(num_classes,))\n",
    "    repeat_class = tfk.layers.RepeatVector(input_shape[0])(class_inputs)\n",
    "    class_combined = tfk.layers.Concatenate(axis=-1)([encoder_lstm, repeat_class])\n",
    "    encoder_lstm_with_class = tfk.layers.LSTM(units=lstm_units_decoder, return_sequences=True)(class_combined)\n",
    "\n",
    "    # First Stage Attention\n",
    "    attention_1 = CBAM_block(encoder_lstm_with_class, lstm_units_decoder)\n",
    "\n",
    "    batchnorm1 = tfkl.BatchNormalization()(attention_1)\n",
    "\n",
    "    # Decoder\n",
    "    repeat_vector = tfk.layers.RepeatVector(input_shape[0])(encoder_lstm_with_class[:, -1, :])\n",
    "    #decoder_inputs = tfk.layers.Concatenate(axis=-1)([attention_1, repeat_vector])\n",
    "    decoder_inputs = tfk.layers.Concatenate(axis=-1)([batchnorm1, repeat_vector])\n",
    "    decoder_lstm = tfk.layers.LSTM(units=lstm_units_decoder, return_sequences=True)(decoder_inputs)\n",
    "\n",
    "    # Second Stage Attention\n",
    "    attention_2 = CBAM_block(decoder_lstm, lstm_units_decoder)\n",
    "\n",
    "    flatten = tfk.layers.Flatten()(attention_2)\n",
    "\n",
    "    # Output layer\n",
    "    output = tfk.layers.Dense(output_shape[0])(flatten)\n",
    "    output = tfk.layers.Activation('linear')(output)\n",
    "\n",
    "\n",
    "    model = tfk.models.Model(inputs=[encoder_inputs, class_inputs], outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJE5WlvUOi1D"
   },
   "outputs": [],
   "source": [
    "model = da_rnn_CBAM_classes(input_shape, output_shape, 128, 64, 64) #change again second 128 to 64\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JUTTgD4Oi1G"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = [X_train, classes_train],\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 25,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/SubmissionModel_46',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxRFCXcPOi1G"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cArOOWy1Oi1I"
   },
   "outputs": [],
   "source": [
    "model = tfk.models.load_model('Models/SubmissionModel_46')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Evaluation"
   ],
   "metadata": {
    "id": "YDTxGtGo4gei"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84qt1SMxOi1J"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Predict the test set using the model\n",
    "predictions = model.predict([X_test, classes_test], verbose=0)\n",
    "\n",
    "for pred in predictions:\n",
    "  for i in range(len(pred)):\n",
    "    if np.isnan(pred[i]):\n",
    "      pred[i] = 0\n",
    "\n",
    "predictions = predictions[:, :9]\n",
    "predictions = np.expand_dims(predictions, axis = -1)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = np.mean(np.square(y_test[:,:9]-predictions))\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test[:, :9].flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")\n",
    "\n",
    "# Calculate and print Root Mean Squared Error (RMSE)\n",
    "root_mean_squared_error = tf.math.sqrt(mean_squared_error)\n",
    "print(f\"Root Mean Squared Error: {root_mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Percentage Error (MAPE)\n",
    "mean_absolute_percentage_error = tfk.metrics.mean_absolute_percentage_error(y_test[:, :9].flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error}\")\n",
    "\n",
    "# Calculate and print Coefficient of Determination (R^2)\n",
    "r2 = r2_score(y_test.flatten(), predictions.flatten())\n",
    "print(f\"Coefficient of Determination (R^2): {r2}\")\n",
    "\n",
    "# Calculate and print Adjusted Coefficient of Determination (Adj. R^2)\n",
    "adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print(f\"Adjusted Coefficient of Determination (Adj. R^2): {adjusted_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwBvWCmjqPMr"
   },
   "source": [
    "# Hypertuning weighted output between two best models, using our test set as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3yH6mMUxAGh"
   },
   "outputs": [],
   "source": [
    "model1 = tfk.models.load_model('Models/SubmissionModel_46')\n",
    "model2 = tfk.models.load_model('Models/SubmissionModel_49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8J9yuMC1V8b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate predictions with given alpha value\n",
    "def calculate_predictions(alpha, out1, out2, classes_test):\n",
    "    for i in range(len(out1)):\n",
    "        if classes_test[i][0] == 1 or classes_test[i][4] == 1:\n",
    "            out1[i] *= alpha\n",
    "        else:\n",
    "            out1[i] *= 1 - alpha\n",
    "\n",
    "    for i in range(len(out2)):\n",
    "        if classes_test[i][0] == 1 or classes_test[i][4] == 1:\n",
    "            out2[i] *= 1 - alpha\n",
    "        else:\n",
    "            out2[i] *= alpha\n",
    "\n",
    "    return out1 + out2\n",
    "\n",
    "# Hyperparameter grid for alpha\n",
    "param_grid = {'alpha': np.linspace(0, 1, 100)}\n",
    "\n",
    "# Initialize best values\n",
    "best_alpha = None\n",
    "best_mse = float('inf')\n",
    "mse_values = []  # To store MSE values for each alpha\n",
    "\n",
    "\n",
    "# Loop through the hyperparameter grid\n",
    "for params in ParameterGrid(param_grid):\n",
    "    alpha = params['alpha']\n",
    "    print(\"\\n\" + \"+\"*50)\n",
    "    print(f'Alpha = {alpha}')\n",
    "\n",
    "    # Calculate predictions\n",
    "    out1 = model1.predict([X_test, classes_test])\n",
    "    out2 = model2.predict([X_test, classes_test])\n",
    "    predictions = calculate_predictions(alpha, out1, out2, classes_test)\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test.flatten(), predictions.flatten())\n",
    "    mse_values.append(mse)\n",
    "\n",
    "\n",
    "\n",
    "    improvement_status = \"Improved!\" if mse < best_mse else \"Not Improved\"\n",
    "\n",
    "    # Update best values if necessary\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_alpha = alpha\n",
    "\n",
    "    # Print details\n",
    "    print(f'Mean Squared Error: {mse} ({improvement_status})')\n",
    "\n",
    "# Print the best hyperparameters and corresponding MSE\n",
    "print(\"\\n\" + \"+\"*50)\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f'  Alpha: {best_alpha}')\n",
    "print(f'  Mean Squared Error: {best_mse}')\n",
    "print(\"+\"*50)\n",
    "\n",
    "\n",
    "# Plot the MSE values as a function of alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_grid['alpha'], mse_values, marker='o')\n",
    "plt.title('Mean Squared Error as a function of Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (Temptative) All Tested Models"
   ],
   "metadata": {
    "id": "0eR0YP6521QX"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiMa-BIV38-A"
   },
   "source": [
    "## ResNet + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snbXlosXVbt7"
   },
   "outputs": [],
   "source": [
    "def CBAM_block(in_block, ch, ratio=16):\n",
    "    # 1. Channel attention block\n",
    "    avg_pool = tf.reduce_mean(in_block, axis=1, keepdims=True)\n",
    "    max_pool = tf.reduce_max(in_block, axis=1, keepdims=True)\n",
    "\n",
    "    dense1 = tfkl.Dense(units=ch // ratio, activation='relu')\n",
    "    avg_reduced = dense1(avg_pool)\n",
    "    max_reduced = dense1(max_pool)\n",
    "\n",
    "    dense2 = tfkl.Dense(units=ch, activation='sigmoid')\n",
    "    avg_attention = dense2(avg_reduced)\n",
    "    max_attention = dense2(max_reduced)\n",
    "\n",
    "    x = tf.add(avg_attention, max_attention)\n",
    "    x = tf.nn.sigmoid(x)\n",
    "    x = tf.multiply(in_block, x)\n",
    "\n",
    "    # 2. Spatial attention block\n",
    "    y_mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    y_max = tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "\n",
    "    y = tf.concat([y_mean, y_max], axis=-1)\n",
    "    y = tfkl.Conv1D(filters=1, kernel_size=5, padding='same', activation='sigmoid')(y)\n",
    "    y = tf.multiply(x, y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def ResBs_CBAM_ConvLSTM(block_input, num_filters):\n",
    "    block_input_short = tfkl.Conv1D(num_filters, kernel_size=1, padding='same')(block_input)\n",
    "    block_input_short = tfkl.BatchNormalization()(block_input_short)\n",
    "\n",
    "    conv1 = tfkl.Conv1D(filters=num_filters, kernel_size=5, strides=2, padding='same')(block_input)\n",
    "    norm1 = tfkl.BatchNormalization()(conv1)\n",
    "    relu1 = tfkl.Activation('relu')(norm1)\n",
    "\n",
    "    lstm = tfkl.LSTM(units=num_filters, return_sequences=True)(relu1)\n",
    "\n",
    "    conv2 = tfkl.Conv1D(num_filters, kernel_size=7, padding='same')(lstm)\n",
    "    norm2 = tfkl.BatchNormalization()(conv2)\n",
    "\n",
    "    CBAM_output = CBAM_block(norm2, num_filters, ratio=16)\n",
    "    block_input_short = tfkl.Conv1D(num_filters, kernel_size=1, padding='same')(block_input_short)\n",
    "\n",
    "    # Adjust the following line to ensure compatibility with input shapes\n",
    "    res_output = tfkl.Add()([tfkl.Cropping1D(cropping=(0, block_input_short.shape[1] - CBAM_output.shape[1]))(block_input_short), CBAM_output])\n",
    "    relu2 = tfkl.Activation('relu')(res_output)\n",
    "\n",
    "    return relu2\n",
    "\n",
    "def ResBs_CBAM_IdentityLSTM(block_input, num_filters):\n",
    "\n",
    "    conv1 = tfkl.Conv1D(filters=num_filters, kernel_size=7, padding='same')(block_input)\n",
    "    norm1 = tfkl.BatchNormalization()(conv1)\n",
    "    relu1 = tfkl.Activation('relu')(norm1)\n",
    "    CBAM_output = CBAM_block(relu1, num_filters, ratio=16)\n",
    "\n",
    "\n",
    "    # Adjust the following line to ensure compatibility with input shapes\n",
    "    res_output = tfkl.Add()([tfkl.Cropping1D(cropping=(0, block_input.shape[1] - CBAM_output.shape[1]))(block_input), CBAM_output])\n",
    "    relu2 = tfkl.Activation('relu')(res_output)\n",
    "\n",
    "    return relu2\n",
    "\n",
    "def resnet_CBAM_LSTM_time_series(input_shape, output_shape, N):\n",
    "    # Input shape for time series data\n",
    "    ts_input = tfkl.Input(shape=input_shape, name='time_series_input')\n",
    "\n",
    "    ResNet = tfkl.Conv1D(filters=64, kernel_size=15, padding='same')(ts_input)\n",
    "    ResNet = tfkl.BatchNormalization()(ResNet)\n",
    "    ResNet = tfkl.Activation('relu')(ResNet)\n",
    "    ResNet = tfkl.MaxPooling1D(pool_size=2, strides=2)(ResNet)\n",
    "\n",
    "    filters = 64\n",
    "    M = int((N - 2) / 2)\n",
    "    for i in range(M):\n",
    "        filters = filters * 2\n",
    "        ResNet = ResBs_CBAM_ConvLSTM(ResNet, filters)\n",
    "        ResNet = ResBs_CBAM_IdentityLSTM(ResNet, filters)\n",
    "\n",
    "    ResNet = tfkl.GlobalAveragePooling1D(name='gap_layer')(ResNet)\n",
    "\n",
    "    # Output layer for time series forecasting (single value prediction)\n",
    "    output = tfkl.Dense(output_shape[0], activation='linear', name='output')(ResNet)\n",
    "\n",
    "    model = tfk.Model(inputs=ts_input, outputs=output)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyYQQ3kwWkIm"
   },
   "outputs": [],
   "source": [
    "model = resnet_CBAM_LSTM_time_series(input_shape, output_shape, N = 4)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pP5r5knu7nm"
   },
   "source": [
    "hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfDjRBEiz7eP"
   },
   "outputs": [],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3OvHJ3wzZu8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.models import Sequential\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'ch': [8, 16, 32],\n",
    "    'N': [4, 6, 8],\n",
    "    'input_shape': [input_shape],\n",
    "    'output_shape': [output_shape]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "model = KerasRegressor(build_fn=resnet_CBAM_LSTM_time_series, N=4, ch = 8, input_shape = input_shape, output_shape = output_shape, epochs=10, batch_size=32, verbose=0)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose = 3)\n",
    "\n",
    "# Assuming y_train has shape (num_samples, num_time_steps, num_features)\n",
    "y_train_reshaped = y_train.reshape(y_train.shape[0], -1)\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_result = grid.fit(X_train, y_train_reshaped)\n",
    "\n",
    "# Print the best parameters and corresponding MSE\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7qQbzQrjU4i"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/Model_RESNET_LSTM',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLUlKfK0CGDg"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TD4twq6I5x60"
   },
   "outputs": [],
   "source": [
    "model.save('Models/Model_RESNET_LSTM')\n",
    "model = tfk.models.load_model('Models/Model_RESNET_LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dra5HA3nYq3_"
   },
   "source": [
    "###Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIoR3vpMom4z"
   },
   "outputs": [],
   "source": [
    "# Predict the test set using the model\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = tfk.metrics.mean_squared_error(y_test[:9, ].flatten(), predictions[:9, ].flatten()).numpy()\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test[:9, ].flatten(), predictions[:9, ].flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQ6wcaQ45Lka"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.plot(X_test.T[0], label='Train')\n",
    "plt.plot(y_test.T[0], label='Test')\n",
    "plt.plot(predictions[0], label='Predicted')\n",
    "plt.title('Test Split')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsqZHW-XAHCi"
   },
   "source": [
    "## ResNet + Squeeze & Excitation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8nFplQUAZMZ"
   },
   "outputs": [],
   "source": [
    "# Squeeze and excitation module\n",
    "def se_block(in_block, ch, ratio=16):\n",
    "    # 1. Define the squeeze block (global pooling)\n",
    "    squeeze = tfkl.GlobalAveragePooling1D()(in_block)\n",
    "    # 2. Define the excitation block (ch/ratio to compress the information)\n",
    "    excitation = tfkl.Dense(units = ch/ratio, activation = 'relu')(squeeze)\n",
    "    # 3. Expand the information and mormalize with a sigmoid (give more information to some channel)\n",
    "    excitation = tfkl.Dense(units = ch, activation='sigmoid')(excitation)\n",
    "    # 4. Multiplication layer.\n",
    "    scaled_input = tfkl.multiply([in_block, excitation])\n",
    "\n",
    "    return scaled_input\n",
    "\n",
    "# ResNet with Squeeze and excitation block model definition\n",
    "## ADD the SQ block\n",
    "\n",
    "def ResBs_SE_Conv(block_input, num_filters):\n",
    "\n",
    "    # 0. Filter Block input and BatchNormalization\n",
    "    block_input_short = tfkl.Conv1D(num_filters, kernel_size=7, strides=2,  padding = 'valid')(block_input)\n",
    "    block_input_short = tfkl.BatchNormalization()(block_input_short)\n",
    "\n",
    "    # 1. First Convolutional Layer\n",
    "    conv1 = tfkl.Conv1D(filters=num_filters, kernel_size=7, strides=2, padding= 'valid')(block_input)\n",
    "    norm1 = tfkl.BatchNormalization()(conv1)\n",
    "    relu1 = tfkl.Activation('relu')(norm1)\n",
    "    dropout = tfkl.Dropout(0.2)(relu1)\n",
    "\n",
    "    # 2. Second Convolutional Layer\n",
    "    conv2 = tfkl.Conv1D(num_filters, kernel_size=7, padding= 'same')(dropout) #per avere concordanza\n",
    "    norm2 = tfkl.BatchNormalization()(conv2)\n",
    "\n",
    "    # Introduce the squeeze and excitation block (best part: after the second convolutional layer)\n",
    "    # 3. SE (the number of channels is the number of filters of the block)\n",
    "    se = se_block(norm2, ch = num_filters)\n",
    "\n",
    "    # 4. Summing Layer (adding a residual connection)\n",
    "    sum = tfkl.Add()([block_input_short, se])\n",
    "\n",
    "    # 5. Activation Layer\n",
    "    relu2 = tfkl.Activation('relu')(sum)\n",
    "\n",
    "    return relu2\n",
    "\n",
    "def ResBs_SE_Identity(block_input, num_filters):\n",
    "\n",
    "    # 1. First Convolutional Layer\n",
    "    conv1 = tfkl.Conv1D(filters=num_filters, kernel_size=7, padding= 'same')(block_input)\n",
    "    norm1 = tfkl.BatchNormalization()(conv1)\n",
    "    relu1 = tfkl.Activation('relu')(norm1)\n",
    "    dropout = tfkl.Dropout(0.2)(relu1)\n",
    "\n",
    "    # 2. Second Convolutional Layer\n",
    "    conv2 = tfkl.Conv1D(num_filters, kernel_size=7, padding= 'same')(dropout) #per avere concordanza\n",
    "    norm2 = tfkl.BatchNormalization()(conv2)\n",
    "\n",
    "    # Introduce the squeeze and excitation block (best part: after the second convolutional layer)\n",
    "    # 3. SE (the number of channels is the number of filters of the block)\n",
    "    se = se_block(norm2, ch = num_filters)\n",
    "\n",
    "    # 4. Summing Layer (adding a residual connection)\n",
    "    sum = tfkl.Add()([block_input, se])\n",
    "\n",
    "    # 5. Activation Layer\n",
    "    relu2 =tfkl. Activation('relu')(sum)\n",
    "\n",
    "    return relu2\n",
    "\n",
    "# model integrating deep + wide\n",
    "def resnet_SE_deep_wide(input_shape, output_shape, N=8):\n",
    "    input = tfkl.Input(shape=input_shape, name='ecg_signal')\n",
    "\n",
    "    ResNet = tfkl.Conv1D(filters=64,kernel_size=15, padding = 'same')(input)\n",
    "    ResNet = tfkl.BatchNormalization()(ResNet)\n",
    "    ResNet = tfkl.Activation('relu')(ResNet)\n",
    "    ResNet = tfkl.MaxPooling1D(pool_size=2, strides = 2)(ResNet)\n",
    "\n",
    "    # B.5 ResBs (x8) blocks\n",
    "    # The number of filters starts from 64 and doubles every two blocks\n",
    "\n",
    "    # First two ResNet blocks are identity blocks\n",
    "    ResNet = ResBs_SE_Identity(ResNet, 64)\n",
    "    ResNet = ResBs_SE_Identity(ResNet, 64)\n",
    "\n",
    "    filters = 64\n",
    "    M= int((N -2 )/2)\n",
    "    for i in range(M):\n",
    "        filters = filters*2\n",
    "\n",
    "        # define N-th ResBs block\n",
    "        ResNet = ResBs_SE_Conv(ResNet, filters)\n",
    "        ResNet = ResBs_SE_Identity(ResNet, filters)\n",
    "\n",
    "    ResNet = tfkl.GlobalAveragePooling1D(name='gap_layer')(ResNet)\n",
    "\n",
    "    # Output layer for time series forecasting (single value prediction)\n",
    "    output = tfkl.Dense(output_shape[0], activation='linear', name='output')(ResNet)\n",
    "\n",
    "    model = tfk.Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvlTfOjuBUCO"
   },
   "outputs": [],
   "source": [
    "model = resnet_SE_deep(input_shape, output_shape, N = 6)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9cVJj8YCLJI"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/Model_RESNET_LSTM',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLj437YnCQqt"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8K-FSUwJAuf"
   },
   "source": [
    "## ResNet + Squeeze & Excitation + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUHl3H86JGr9"
   },
   "outputs": [],
   "source": [
    "# Squeeze and excitation module\n",
    "def se_block(in_block, ch, ratio=16):\n",
    "    # 1. Define the squeeze block (global pooling)\n",
    "    squeeze = tfkl.GlobalAveragePooling1D()(in_block)\n",
    "    # 2. Define the excitation block (ch/ratio to compress the information)\n",
    "    excitation = tfkl.Dense(units = ch/ratio, activation = 'relu')(squeeze)\n",
    "    # 3. Expand the information and mormalize with a sigmoid (give more information to some channel)\n",
    "    excitation = tfkl.Dense(units = ch, activation='sigmoid')(excitation)\n",
    "    # 4. Multiplication layer.\n",
    "    scaled_input = tfkl.multiply([in_block, excitation])\n",
    "\n",
    "    return scaled_input\n",
    "\n",
    "# ResNet with Squeeze and excitation block model definition\n",
    "## ADD the SQ block\n",
    "\n",
    "def ResBs_SE_LSTM(block_input, num_filters):\n",
    "\n",
    "    # 0. Filter Block input and BatchNormalization\n",
    "    block_input_short = tfkl.Conv1D(num_filters, kernel_size=1, padding='same')(block_input)  # Adjusted Conv1D layer for shape matching\n",
    "    block_input_short = tfkl.BatchNormalization()(block_input_short)\n",
    "\n",
    "    # 1. LSTM layer instead of Conv1D\n",
    "    lstm = tfkl.LSTM(units=num_filters, return_sequences=True)(block_input)\n",
    "    lstm = tfkl.Conv1D(num_filters, kernel_size=1, padding='same')(lstm)  # Adjusted Conv1D layer for shape matching\n",
    "    norm1 = tfkl.BatchNormalization()(lstm)\n",
    "    relu1 = tfkl.Activation('relu')(norm1)\n",
    "    dropout = tfkl.Dropout(0.2)(relu1)\n",
    "\n",
    "    # Rest of your implementation remains the same...\n",
    "    # 2. Second Convolutional Layer\n",
    "    conv2 = tfkl.Conv1D(num_filters, kernel_size=7, padding='same')(dropout)\n",
    "    norm2 = tfkl.BatchNormalization()(conv2)\n",
    "\n",
    "    # Introduce the squeeze and excitation block (best part: after the second convolutional layer)\n",
    "    # 3. SE (the number of channels is the number of filters of the block)\n",
    "    se = se_block(norm2, ch=num_filters)\n",
    "\n",
    "    # 4. Summing Layer (adding a residual connection)\n",
    "    sum_layer = tfkl.Add()([block_input_short, se])\n",
    "\n",
    "    # 5. Activation Layer\n",
    "    relu2 = tfkl.Activation('relu')(sum_layer)\n",
    "\n",
    "    return relu2\n",
    "\n",
    "# Modify your resnet_SE_deep_wide function to incorporate the LSTM-based block\n",
    "\n",
    "def resnet_SE_deep_LSTM(input_shape, output_shape, N=8):\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='ecg_signal')\n",
    "\n",
    "    ResNet = tfkl.Conv1D(filters=64, kernel_size=15, padding='same')(input_layer)\n",
    "    ResNet = tfkl.BatchNormalization()(ResNet)\n",
    "    ResNet = tfkl.Activation('relu')(ResNet)\n",
    "    ResNet = tfkl.MaxPooling1D(pool_size=2, strides=2)(ResNet)\n",
    "\n",
    "    # First two ResNet blocks are identity blocks\n",
    "    ResNet = ResBs_SE_Identity(ResNet, 64)\n",
    "    ResNet = ResBs_SE_Identity(ResNet, 64)\n",
    "\n",
    "    filters = 64\n",
    "    M = int((N - 2) / 2)\n",
    "    for i in range(M):\n",
    "        filters = filters * 2\n",
    "\n",
    "        # Use the LSTM-based block\n",
    "        ResNet = ResBs_SE_LSTM(ResNet, filters)\n",
    "        ResNet = ResBs_SE_Identity(ResNet, filters)\n",
    "\n",
    "    ResNet = tfkl.GlobalAveragePooling1D(name='gap_layer')(ResNet)\n",
    "\n",
    "    # Output layer for time series forecasting (single value prediction)\n",
    "    output_layer = tfkl.Dense(output_shape[0], activation='linear', name='output')(ResNet)\n",
    "\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u55Hj9ydJGr_"
   },
   "outputs": [],
   "source": [
    "model = resnet_SE_deep(input_shape, output_shape, N = 6)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO4mAMQ9JGr_"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/Model_RESNET_LSTM',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppnUDCaUJGsA"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8_dslrPJGsB"
   },
   "outputs": [],
   "source": [
    "# Predict the test set using the model\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = tfk.metrics.mean_squared_error(y_test[:9, ].flatten(), predictions[:9, ].flatten()).numpy()\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test[:9, ].flatten(), predictions[:9, ].flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (Temptative) Transfer Learning From Best Model"
   ],
   "metadata": {
    "id": "LKU2ZU7yHV-z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the best model\n",
    "best_model = tfk.models.load_model('Models/SubmissionModel_46')"
   ],
   "metadata": {
    "id": "TXVUYrClHbM5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Feez the layers of the model\n",
    "for layer in best_model.layers:\n",
    "    layer.trainable = False\n"
   ],
   "metadata": {
    "id": "-AFKvX_mH57x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First model (no dense)"
   ],
   "metadata": {
    "id": "ta5LzsMPQYAR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming best_model is a Sequential or Functional model\n",
    "# Get all layers except the last one from best_model\n",
    "modified_best_model = tfk.Model(inputs=best_model.inputs, outputs=best_model.layers[-2].output)\n",
    "\n",
    "# Add a new Dense layer with neurons specified by output_shape\n",
    "modified_best_model_output = tfkl.Dense(output_shape[0], activation='linear')(modified_best_model.output)\n",
    "\n",
    "# Create the final model by defining inputs and outputs\n",
    "final_model_1 = tfk.Model(inputs=modified_best_model.inputs, outputs=modified_best_model_output)\n",
    "\n",
    "# Compile the final model\n",
    "final_model_1.compile(optimizer='adam', loss='mse')"
   ],
   "metadata": {
    "id": "YoHZdOoCH-Tj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second Model (with Dense)"
   ],
   "metadata": {
    "id": "-HZg4fFuQcmD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "modified_best_model = tfk.Model(inputs=best_model.inputs, outputs=best_model.layers[-2].output)\n",
    "\n",
    "# Add a new Dense layer with 16 neurons between modified_best_model and the output\n",
    "dense_layer = tfkl.Dense(64, activation='relu')(modified_best_model.output)\n",
    "\n",
    "# Add another Dense layer for the final output with neurons specified by output_shape\n",
    "final_output = tfkl.Dense(output_shape[0], activation='linear')(dense_layer)\n",
    "\n",
    "# Create the final model by defining inputs and outputs\n",
    "final_model_2 = tfk.Model(inputs=modified_best_model.inputs, outputs=final_output)\n",
    "\n",
    "# Compile the final model\n",
    "final_model_2.compile(optimizer='adam', loss='mse')"
   ],
   "metadata": {
    "id": "p1oNdTNEQf24"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "final_model_1.summary()\n",
    "tfk.utils.plot_model(final_model_1, expand_nested=True, show_shapes=True)"
   ],
   "metadata": {
    "id": "ebMbgxwKKpbU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "history = final_model_1.fit(\n",
    "    x = [X_train, classes_train],\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 15,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/Model_full_transfer_learning',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ").history"
   ],
   "metadata": {
    "id": "rKDymvmNJA53"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeSdrMnnLeSV"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ayz_lVpULeSa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Predict the test set using the model\n",
    "predictions = final_model_1.predict([X_test, classes_test], verbose=0)\n",
    "\n",
    "for pred in predictions:\n",
    "  for i in range(len(pred)):\n",
    "    if np.isnan(pred[i]):\n",
    "      pred[i] = 0\n",
    "\n",
    "predictions = np.expand_dims(predictions, axis = -1)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = np.mean(np.square(y_test-predictions))\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")\n",
    "\n",
    "# Calculate and print Root Mean Squared Error (RMSE)\n",
    "root_mean_squared_error = tf.math.sqrt(mean_squared_error)\n",
    "print(f\"Root Mean Squared Error: {root_mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Percentage Error (MAPE)\n",
    "mean_absolute_percentage_error = tfk.metrics.mean_absolute_percentage_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error}\")\n",
    "\n",
    "# Calculate and print Coefficient of Determination (R^2)\n",
    "r2 = r2_score(y_test.flatten(), predictions.flatten())\n",
    "print(f\"Coefficient of Determination (R^2): {r2}\")\n",
    "\n",
    "# Calculate and print Adjusted Coefficient of Determination (Adj. R^2)\n",
    "adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print(f\"Adjusted Coefficient of Determination (Adj. R^2): {adjusted_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autocorrelation Model"
   ],
   "metadata": {
    "id": "vfUaVNcVkGoa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHxkPlVHSsa9"
   },
   "outputs": [],
   "source": [
    "def autocorrelation(X, lags):\n",
    "    autocorrelations = []\n",
    "\n",
    "    for data in X:\n",
    "        mean = np.mean(data)\n",
    "        var = np.var(data)\n",
    "        ndata = np.array(data) - mean\n",
    "\n",
    "        acorr = np.zeros(len(lags))\n",
    "\n",
    "        for l in lags:\n",
    "            c = 1.0  # Self correlation\n",
    "\n",
    "            if l > 0:\n",
    "                tmp = ndata[l:] * ndata[:-l]\n",
    "                if var == 0:\n",
    "                  c = 0\n",
    "                else:\n",
    "                  c = np.sum(tmp) / (len(data) - l) / var\n",
    "\n",
    "            acorr[l] = c\n",
    "\n",
    "        autocorrelations.append(acorr)\n",
    "\n",
    "    return autocorrelations\n",
    "# Assuming lag_max is defined as the maximum lag to consider\n",
    "lag_max = range(10)\n",
    "autocorr_features_train = autocorrelation(X_train, lag_max)\n",
    "autocorr_features_train = np.expand_dims(autocorr_features_train, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnOmgoZ1YA3q"
   },
   "outputs": [],
   "source": [
    "autocorr_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gimNuSfqSma2"
   },
   "outputs": [],
   "source": [
    "def da_rnn_with_autocorr(input_shape, output_shape, lstm_units_encoder=64, lstm_units_decoder=64, dense_units=64, autocorr_features=None):\n",
    "    # Encoder\n",
    "    encoder_inputs = tfkl.Input(shape=input_shape)\n",
    "    encoder_lstm = tfkl.LSTM(units=lstm_units_encoder, return_sequences=True)(encoder_inputs)\n",
    "\n",
    "    # Autocorrelation branch\n",
    "    if autocorr_features is not None:\n",
    "        autocorr_inputs = tfkl.Input(shape=(autocorr_features,))\n",
    "        autocorr_dense = tfkl.Dense(units=dense_units, activation='relu')(autocorr_inputs)\n",
    "        repeat_autocorr = tfkl.RepeatVector(input_shape[0])(autocorr_dense)\n",
    "        autocorr_combined = tfkl.Concatenate(axis=-1)([encoder_lstm, repeat_autocorr])\n",
    "        encoder_lstm = tfkl.LSTM(units=lstm_units_decoder, return_sequences=True)(autocorr_combined)\n",
    "\n",
    "    # First Stage Attention\n",
    "    attention_1 = tfkl.Dense(input_shape[0], activation='softmax')(encoder_lstm)\n",
    "    attention_1 = tfkl.Permute((2, 1))(attention_1)\n",
    "    attention_1 = tfkl.Dot(axes=(2, 1))([attention_1, encoder_lstm])\n",
    "\n",
    "    # Decoder\n",
    "    repeat_vector = tfkl.RepeatVector(input_shape[0])(encoder_lstm[:, -1, :])\n",
    "    decoder_inputs = tfkl.Concatenate(axis=-1)([attention_1, repeat_vector])\n",
    "    decoder_lstm = tfkl.LSTM(units=lstm_units_decoder, return_sequences=True)(decoder_inputs)\n",
    "\n",
    "    # Second Stage Attention\n",
    "    attention_2 = tfkl.Dense(input_shape[0], activation='softmax')(decoder_lstm)\n",
    "    attention_2 = tfkl.Permute((2, 1))(attention_2)\n",
    "    attention_2 = tfkl.Dot(axes=(2, 1))([attention_2, decoder_lstm])\n",
    "\n",
    "    # Additional Dense Layer with Dropout\n",
    "    dense_layer = tfkl.Dense(dense_units, activation='relu')(attention_2)\n",
    "    dense_layer = tfkl.Dropout(0.1)(dense_layer)\n",
    "\n",
    "    flatten = tfkl.Flatten()(dense_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output = tfkl.Dense(output_shape[0])(flatten)\n",
    "    output = tfkl.Activation('linear')(output)\n",
    "\n",
    "    if autocorr_features is not None:\n",
    "        model = tfk.Model(inputs=[encoder_inputs, autocorr_inputs], outputs=output)\n",
    "    else:\n",
    "        model = tfk.Model(inputs=encoder_inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpVOewsHZxRf"
   },
   "outputs": [],
   "source": [
    "model = da_rnn_with_autocorr(input_shape, output_shape, 64, 64, 64, autocorr_features = autocorr_features_train.shape[1])\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzvwUGSTahjz"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = [X_train, autocorr_features_train],\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tfk.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=.1,\n",
    "            patience=5,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tfk.callbacks.ModelCheckpoint(\n",
    "            filepath='Models/Model_autocorrelation',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAlIoyRCbLRe"
   },
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkCu3lNya6FR"
   },
   "outputs": [],
   "source": [
    "model = tfk.models.load_model('Models/Model_autocorrelation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0yb2kP5bLRi"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predict the test set using the model\n",
    "predictions = model.predict([X_test, autocorr_features] , verbose=0)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = tfk.metrics.mean_squared_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")\n",
    "\n",
    "# Calculate and print Root Mean Squared Error (RMSE)\n",
    "root_mean_squared_error = tf.math.sqrt(mean_squared_error)\n",
    "print(f\"Root Mean Squared Error: {root_mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Percentage Error (MAPE)\n",
    "mean_absolute_percentage_error = tfk.metrics.mean_absolute_percentage_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error}\")\n",
    "\n",
    "# Calculate and print Coefficient of Determination (R^2)\n",
    "r2 = r2_score(y_test.flatten(), predictions.flatten())\n",
    "print(f\"Coefficient of Determination (R^2): {r2}\")\n",
    "\n",
    "# Calculate and print Adjusted Coefficient of Determination (Adj. R^2)\n",
    "adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print(f\"Adjusted Coefficient of Determination (Adj. R^2): {adjusted_r2}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ap3o8JayfgEM",
    "a49TrZ82D-Fl",
    "vhuy0CsjSSns",
    "LqnDQfUBbfZa",
    "EHaglE3Ghqnu",
    "iGlHVnkTOi1A",
    "HwBvWCmjqPMr",
    "tiMa-BIV38-A",
    "JsqZHW-XAHCi",
    "C8K-FSUwJAuf",
    "LKU2ZU7yHV-z",
    "vfUaVNcVkGoa"
   ],
   "provenance": [
    {
     "file_id": "1yLXx6xmp8OyZhVSN6PtF-WCP6pW29Cdu",
     "timestamp": 1703153329097
    },
    {
     "file_id": "1WECcx583dyQ_mEZwQKgbIdSSzrruo7A3",
     "timestamp": 1702139701030
    },
    {
     "file_id": "1ZFpK5mnG4HglLl1SUi4pBDeTX_Yr-BRD",
     "timestamp": 1702120573190
    },
    {
     "file_id": "1WzLJV7yax76oEMQNI_FmFaL-P33BuSuN",
     "timestamp": 1702026511569
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
